{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMapQn0-y2Bk"
      },
      "source": [
        "# Q-learning\n",
        "\n",
        "In this notebook, you will implement Q-learning as described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9vApPmQFGUs"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9pkLaPNMOTP",
        "outputId": "9bfc9c55-627f-46d8-ecba-81f13dc56681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (2.1.1)\n",
            "Requirement already satisfied: pandas in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install numpy pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8kdEPNmFOCr"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T9cAvA0GLkXh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sys          # We use sys to get the max value of a float\n",
        "import pandas as pd # We only use pandas for displaying tables nicely\n",
        "pd.options.display.float_format = '{:,.3f}'.format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTNglEH9FR8f"
      },
      "source": [
        "### ```World``` class and globals\n",
        "\n",
        "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
        "\n",
        "A _state_ is a tuple $(x,y)$.\n",
        "\n",
        "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
        "\n",
        "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wMAd6qTASn9u"
      },
      "outputs": [],
      "source": [
        "# Globals:\n",
        "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
        "\n",
        "# Rewards, terminals and obstacles are characters:\n",
        "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
        "TERMINALS = (\"+\", \"-\") # Note a terminal should also have a reward assigned\n",
        "OBSTACLES = (\"#\")\n",
        "\n",
        "# Discount factor\n",
        "gamma = 1\n",
        "\n",
        "# The probability of a random move:\n",
        "rand_move_probability = 0\n",
        "\n",
        "class World:\n",
        "  def __init__(self, width, height):\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    # Create an empty world where the agent can move to all cells\n",
        "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
        "\n",
        "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
        "    \"\"\"\n",
        "    Create an obstacle in either a single cell or rectangle.\n",
        "    \"\"\"\n",
        "    if end_x == None: end_x = start_x\n",
        "    if end_y == None: end_y = start_y\n",
        "\n",
        "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
        "\n",
        "  def add_reward(self, x, y, reward):\n",
        "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
        "    self.grid[x, y] = reward\n",
        "\n",
        "  def add_terminal(self, x, y, terminal):\n",
        "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
        "    self.grid[x, y] = terminal\n",
        "\n",
        "  def is_obstacle(self, x, y):\n",
        "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
        "      return True\n",
        "    else:\n",
        "      return self.grid[x ,y] in OBSTACLES\n",
        "\n",
        "  def is_terminal(self, x, y):\n",
        "    return self.grid[x ,y] in TERMINALS\n",
        "\n",
        "  def get_reward(self, x, y):\n",
        "    \"\"\"\n",
        "    Return the reward associated with a given location\n",
        "    \"\"\"\n",
        "    return REWARDS[self.grid[x, y]]\n",
        "\n",
        "  def get_next_state(self, current_state, action):\n",
        "    \"\"\"\n",
        "    Get the next state given a current state and an action. The outcome can be\n",
        "    stochastic  where rand_move_probability determines the probability of\n",
        "    ignoring the action and performing a random move.\n",
        "    \"\"\"\n",
        "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
        "\n",
        "    x, y = current_state\n",
        "\n",
        "    # If our current state is a terminal, there is no next state\n",
        "    if self.grid[x, y] in TERMINALS:\n",
        "      return None\n",
        "\n",
        "    # Check of a random action should be performed:\n",
        "    if np.random.rand() < rand_move_probability:\n",
        "      action = np.random.choice(ACTIONS)\n",
        "\n",
        "    if action == \"up\":      y -= 1\n",
        "    elif action == \"down\":  y += 1\n",
        "    elif action == \"left\":  x -= 1\n",
        "    elif action == \"right\": x += 1\n",
        "\n",
        "    # If the next state is an obstacle, stay in the current state\n",
        "    return (x, y) if not self.is_obstacle(x, y) else current_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKybFbwCG478"
      },
      "source": [
        "## A simple world"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVpq3vosHAol",
        "outputId": "878e3e18-9def-4cc4-8a9e-c1d77395960f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' '+']]\n"
          ]
        }
      ],
      "source": [
        "world = World(4, 6)\n",
        "\n",
        "# Since we only focus on episodic tasks, we must have a terminal state that the\n",
        "# agent eventually reaches\n",
        "world.add_terminal(3, 5, \"+\")\n",
        "\n",
        "print(world.grid.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK_bFSHsQJYr"
      },
      "source": [
        "## Exercise: Q-learning\n",
        "\n",
        "Implement and test Q-learning. You should be able to base much of your code on your implementation of SARSA. Since Q-learning is an off-policy method, we can use whatever behavior policy we want during training, but the choice of behavioral policy still manners so it is a good idea to balance exploration and exploitation. During testing, we can then use the learnt policy (the target policy).\n",
        "\n",
        "As for the behavior policy, you can use an simple $\\epsilon$-greedy policy, but you can also experiment with alternatives, for instance, optimistic initial values.\n",
        "\n",
        "See page 131 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the Q-learning algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "wEIxyiw1J1fd",
        "outputId": "ab8ec86a-721b-42b4-a671-1bc8736587df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>+</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3\n",
              "0            \n",
              "1            \n",
              "2     +      \n",
              "3            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "      <td>left</td>\n",
              "      <td>down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "      <td>left</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>right</td>\n",
              "      <td>up</td>\n",
              "      <td>left</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>right</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0     1     2     3\n",
              "0  right  down  left  down\n",
              "1  right  down  left  left\n",
              "2  right    up  left  left\n",
              "3  right    up    up    up"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO: Implement your code here -- you need a Q-table to keep track of action\n",
        "#       value estimates and a policy-function that returns an epsilon greedy\n",
        "#       policy based on your estimates.\n",
        "world2 = World(4,4)\n",
        "world2.add_terminal(1, 2, \"+\")\n",
        "EPISODES = 1000\n",
        "epsilon = 0.4\n",
        "alpha = 0.05\n",
        "\n",
        "def epsilon_greedy(state, Q):\n",
        "  x,y = state\n",
        "  if np.random.rand() <= epsilon:\n",
        "    return random.choice(ACTIONS)\n",
        "  else:\n",
        "    return ACTIONS[np.argmax(Q[x,y])]\n",
        "\n",
        "\n",
        "def q_learning(world, start_state):\n",
        "  Q = np.zeros((world.width, world.height,len(ACTIONS)))\n",
        "  for _ in range(EPISODES):\n",
        "    current_state = start_state\n",
        "    current_action = epsilon_greedy(current_state, Q)\n",
        "\n",
        "    action_to_index = {action: i for i, action in enumerate(ACTIONS)}\n",
        "\n",
        "    while not world.is_terminal(current_state[0], current_state[1]):\n",
        "\n",
        "      # Get the next state from the world\n",
        "      next_state = world.get_next_state(current_state, current_action)\n",
        "      next_action = epsilon_greedy(next_state, Q)\n",
        "      best_action = ACTIONS[np.argmax(Q[next_state[0], next_state[1]])]\n",
        "\n",
        "      Q[current_state[0], current_state[1], action_to_index[current_action]] += alpha * (\n",
        "          world.get_reward(*next_state) + gamma * Q[next_state[0], next_state[1], action_to_index[best_action]]\n",
        "        - Q[current_state[0], current_state[1], action_to_index[current_action]])\n",
        "      current_state = next_state\n",
        "      current_action = next_action\n",
        "\n",
        "  return Q\n",
        "\n",
        "Q_q = q_learning(world2, (3, 3))\n",
        "\n",
        "# for x in range(world2.width):\n",
        "#   for y in range(world2.height):\n",
        "#     for action in ACTIONS:\n",
        "#       print(f\"State: {x,y} - Action: {action} - Value: {Q_q[x,y,ACTIONS.index(action)]}\")\n",
        "#     print(\"\\n\")\n",
        "Pol = np.full((world2.width, world2.height),\"right\")\n",
        "\n",
        "for x in range(world2.width):\n",
        "  for y in range(world2.height):\n",
        "    for action in ACTIONS:\n",
        "      #print(f\"State: {x,y} - Action: {action} - Value: {Q[x,y,ACTIONS.index(action)]}\")\n",
        "      Pol[x,y] = ACTIONS[np.argmax(Q_q[x,y])]\n",
        "    #print(\"\\n\")\n",
        "\n",
        "display(pd.DataFrame(world2.grid.T))\n",
        "display(pd.DataFrame(Pol.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMdPGy2dxxRb"
      },
      "source": [
        "## Exercise: Compare Q-learning and SARSA\n",
        "\n",
        "Setup experiments to compare the performance of Q-learning and SARSA. You can use different ```Worlds``` and test different parameter setting, e.g. for $\\alpha$ and $\\epsilon$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "F7y7ImAjWckA",
        "outputId": "5a8709d2-a1f7-49a1-fbb1-85d8b781ecee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "      <td>left</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>right</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0     1     2     3\n",
              "0  right  down  left  left\n",
              "1  right  down    up    up\n",
              "2  right    up    up    up\n",
              "3     up    up    up    up"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "      <td>left</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>right</td>\n",
              "      <td>up</td>\n",
              "      <td>left</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0     1     2     3\n",
              "0  right  down  left  left\n",
              "1   down  down  down  left\n",
              "2  right    up  left  left\n",
              "3     up    up    up    up"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### TODO: Implement your code here\n",
        "pol_q = np.full((world2.width, world2.height),\"right\")\n",
        "pol_s = np.full((world2.width, world2.height),\"right\")\n",
        "\n",
        "def sarsa(world, start_state):\n",
        "  Q = np.zeros((world.width, world.height,len(ACTIONS)))\n",
        "  for episode in range(EPISODES):\n",
        "    current_state = start_state\n",
        "    current_action = epsilon_greedy(current_state, Q)\n",
        "\n",
        "    action_to_index = {action: i for i, action in enumerate(ACTIONS)}\n",
        "\n",
        "    while not world.is_terminal(current_state[0], current_state[1]):\n",
        "\n",
        "      # Get the next state from the world\n",
        "      next_state = world.get_next_state(current_state, current_action)\n",
        "      next_action = epsilon_greedy(next_state, Q)\n",
        "\n",
        "      Q[current_state[0], current_state[1], action_to_index[current_action]] += alpha * (\n",
        "          world.get_reward(*next_state) + gamma *\n",
        "          Q[next_state[0], next_state[1], action_to_index[next_action]] -\n",
        "          Q[current_state[0], current_state[1], action_to_index[current_action]])\n",
        "      current_state = next_state\n",
        "      current_action = next_action\n",
        "  return Q\n",
        "\n",
        "\n",
        "Q_s = sarsa(world2, (0,0))\n",
        "Q_q = q_learning(world2, (0, 0))\n",
        "\n",
        "\n",
        "\n",
        "for x in range(world2.width):\n",
        "  for y in range(world2.height):\n",
        "    for action in ACTIONS:\n",
        "      pol_q[x,y] = ACTIONS[np.argmax(Q_q[x,y])]\n",
        "      pol_s[x,y] = ACTIONS[np.argmax(Q_s[x,y])]\n",
        "\n",
        "\n",
        "display(pd.DataFrame(pol_q.T))\n",
        "display(pd.DataFrame(pol_s.T))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHHfshQ8RtP-"
      },
      "source": [
        "## Optional exercise: Maximization Bias and Double Learning\n",
        "\n",
        "Below is an implementation of the task shown in Example 6.7 on page 134 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). There are two states, ```A``` and ```B``` where the agent can perform actions, and a terminal state ```T```. ```A``` and ```B``` have different actions available:\n",
        "\n",
        "* ```A``` has ```left``` (to ```B```) and ```right``` to the terminal state\n",
        "* ```B``` has a larger number of actions all leading to a terminal state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RSPgN1YRI9dx"
      },
      "outputs": [],
      "source": [
        "# States \"A\" and \"B\" have actions while \"T\" is a terminal state.\n",
        "STATES = (\"A\", \"B\", \"T\")\n",
        "\n",
        "class Example67MDP:\n",
        "    def __init__(self, number_of_B_actions):\n",
        "        \"\"\"\n",
        "        Create an example and set the number of outgoing actions for state \"B\"\n",
        "        (in the book, they do not give a specific number, but merely write that\n",
        "        from \"B\" there \"are many possible actions all of which cause immediate\n",
        "        termination with a reward drawn from a normal distribution with mean\n",
        "        -0.1 and variance 1. So, feel free to play with different number of\n",
        "        actions in state B)\n",
        "\n",
        "        \"\"\"\n",
        "        self.number_of_B_actions = number_of_B_actions\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns the set of actions availabe in a given state (a tuple\n",
        "        with strings).\n",
        "        \"\"\"\n",
        "        assert state in STATES, f\"State must be one of {STATES}, not {state}\"\n",
        "        if state == \"A\":\n",
        "            return (\"left\", \"right\")\n",
        "        if state == \"B\":\n",
        "            return tuple(f\"{i}\" for i in range(self.number_of_B_actions))\n",
        "        if state == \"T\":\n",
        "            return tuple(\"N\")\n",
        "\n",
        "    def get_next_state_and_reward(self, state, action):\n",
        "        \"\"\"\n",
        "        Get the next state and reward given a current state and an action\n",
        "        \"\"\"\n",
        "        assert state in STATES, f\"Unknown state: {state}\"\n",
        "        assert action in self.get_actions(state), f\"Unknown action {action} for state {state}\"\n",
        "\n",
        "        if state == \"T\":\n",
        "            raise Exception(\"The terminal state has no actions and no next state\")\n",
        "\n",
        "        if state == \"A\":\n",
        "            if action == \"right\":\n",
        "                return \"T\", 0\n",
        "            if action == \"left\":\n",
        "                return \"B\", 0\n",
        "\n",
        "        if state == \"B\":\n",
        "            return \"T\", np.random.normal(loc = -0.1)\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        assert state in STATES, f\"Unknown state: {state}\"\n",
        "        return state == \"T\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ippehOfaxbVu"
      },
      "source": [
        "Implement Double Q-learning (see page 136 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)) and test it on the ```Example67MDP``` above. Notice, that the number of actions differs between the two states ($\\mathcal{A}$(```\"A\"```) $\\neq \\mathcal{A}$(```\"B\"```)), which you have to take into account in your Q-tables. See the code for ```Example67MDP``` above: you can get the set of actions available in a given state by calling ```get_actions(...)``` with the state as argument.\n",
        "\n",
        "Compare action-value estimates for ```\"left\"``` and ```\"right\"``` in state ```\"A\"```  at different times during learning when using double-Q learning and when using normal Q-learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x2YL07t5yiUe"
      },
      "outputs": [],
      "source": [
        "# Create an instance of Example 6.7 with 10 actions in B\n",
        "example = Example67MDP(10)\n",
        "\n",
        "gamma = 1\n",
        "alpha = 0.05\n",
        "\n",
        "# Create two Q-tables (feel free to use your own representation):\n",
        "Q1 = [[0 for _ in range(len(example.get_actions(state)))] for state in STATES]\n",
        "Q2 = [[0 for _ in range(len(example.get_actions(state)))] for state in STATES]\n",
        "\n",
        "# Uncomment to disable double-Q-learning:\n",
        "#Q2 = Q1\n",
        "\n",
        "# You can use the below policy method if you use the Q1 and Q2 as defined above.\n",
        "# If you have done your own representation, you probably have to modify or\n",
        "# rewrite the function below:\n",
        "\n",
        "def e_greedy_dql_policy(state):\n",
        "  global example\n",
        "  actions = { a:epsilon/len(example.get_actions(state)) for a in example.get_actions(state) }\n",
        "  # Do a Q1 + Q2 to do epsilon greedy based on both tables:\n",
        "  Q = [sum(x) for x in zip(Q1[STATES.index(state)], Q2[STATES.index(state)])]\n",
        "  actions[example.get_actions(state)[np.argmax(Q)]] = 1 - epsilon + epsilon/len(example.get_actions(state))\n",
        "  return actions\n",
        "\n",
        "### TODO: Implement double Q-learning\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
