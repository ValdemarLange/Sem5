{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMapQn0-y2Bk"
   },
   "source": [
    "# Temporal difference prediction and control\n",
    "\n",
    "In this notebook, you will implement temporal difference approaches to prediction and control described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9vApPmQFGUs"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9pkLaPNMOTP",
    "outputId": "11ba3071-b7e8-4fe1-9b20-042a45d0b8a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: pandas in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8kdEPNmFOCr"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T9cAvA0GLkXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys          # We use sys to get the max value of a float\n",
    "import pandas as pd # We only use pandas for displaying tables nicely\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTNglEH9FR8f"
   },
   "source": [
    "### ```World``` class and globals\n",
    "\n",
    "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wMAd6qTASn9u"
   },
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\") \n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\") # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:  \n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    # Create an empty world where the agent can move to all cells\n",
    "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
    "  \n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "    \n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def add_terminal(self, x, y, terminal):\n",
    "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "    self.grid[x, y] = terminal\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES \n",
    "\n",
    "  def is_terminal(self, x, y):\n",
    "    return self.grid[x ,y] in TERMINALS\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\" \n",
    "    Return the reward associated with a given location\n",
    "    \"\"\" \n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. The outcome can be\n",
    "    stochastic  where rand_move_probability determines the probability of \n",
    "    ignoring the action and performing a random move.\n",
    "    \"\"\"    \n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "    \n",
    "    x, y = current_state \n",
    "    \n",
    "    # If our current state is a terminal, there is no next state\n",
    "    if self.grid[x, y] in TERMINALS:\n",
    "      return None\n",
    "\n",
    "    # Check of a random action should be performed:\n",
    "    if np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    elif action == \"rightd\": x += 1; y += 1\n",
    "    elif action == \"rightu\": x += 1; y -= 1\n",
    "    elif action == \"leftd\": x -= 1; y += 1\n",
    "    elif action == \"leftu\": x -= 1; y -= 1\n",
    "\n",
    "    # If the next state is an obstacle, stay in the current state\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKybFbwCG478"
   },
   "source": [
    "## A simple world and a simple policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVpq3vosHAol",
    "outputId": "eeea5492-4a17-4907-f283-ee1e54f2e84f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ']\n",
      " [' ' ' ']\n",
      " [' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "world = World(2, 3)\n",
    "\n",
    "# Since we only focus on episodic tasks, we must have a terminal state that the \n",
    "# agent eventually reaches\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "\n",
    "def equiprobable_random_policy(x, y):\n",
    "  return { k:1/len(ACTIONS) for k in ACTIONS }\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxncUHvz1_Hb"
   },
   "source": [
    "## Exercise: TD prediction\n",
    "\n",
    "You should implement TD prediction for estimating $V≈v_\\pi$. See page 120 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n",
    "\n",
    "\n",
    "To implement TD prediction, the agent has to interact with the world for a certain number of episodes. However, unlike in the Monte Carlo case, we do not rely on complete sample runs, but instead update estimates (for prediction and control) and the policy (for control only) each time step in an episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfMg_Zjl40vx"
   },
   "source": [
    "Below, you can see the code for running an episode, with a TODO where you have to add your code for prediction. Also, play with the parameters ```alpha``` and ```EPISODES```, you will typically need a lot more than 10 episodes for an agent to learn anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51rP-8eH4w1V",
    "outputId": "013c329a-c70f-448d-96ae-c68d594208e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.400</td>\n",
       "      <td>3.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.481</td>\n",
       "      <td>5.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.635</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0 3.400 3.820\n",
       "1 4.481 5.384\n",
       "2 6.635 0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Global variable to keep track of current estimates\n",
    "V = np.full((world.width, world.height), 0.0) # TODO\n",
    "\n",
    "# Our step size / learing rate \n",
    "alpha = 0.05 ##### SÆT DEN HER NED FOR AT KOMME NÆRMERE TRUE VALUE!!!!!!!!!!\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Episodes to run \n",
    "EPISODES = 1000\n",
    "\n",
    "def TD_prediction_run_episode(world, policy, start_state):\n",
    "    current_state = start_state\n",
    "    while not world.is_terminal(*current_state):\n",
    "        # Get the possible actions and their probabilities that our policy says \n",
    "        # that the agent should perform in the current state: \n",
    "        possible_actions = policy(*current_state)\n",
    "\n",
    "        # Pick a weighted random action: \n",
    "        action = random.choices(population=list(possible_actions.keys()), \n",
    "                                weights=possible_actions.values(), k=1)  \n",
    "        \n",
    "        # Get the next state from the world\n",
    "        next_state = world.get_next_state(current_state, action[0])\n",
    "        \n",
    "        # Get the reward for performing the action\n",
    "        reward = world.get_reward(*next_state)\n",
    "\n",
    "        ###TODO: =============================================================\n",
    "        ###TODO: Substitute the next line of code with your own\n",
    "        ###TODO: =============================================================\n",
    "        V[current_state] = V[current_state] + alpha * (reward + gamma * V[next_state] - V[current_state])\n",
    "\n",
    "        # print(f\"Current state (S) = {current_state}, next_state S' = {next_state}, reward = {reward}\")\n",
    "\n",
    "        # Move the agent to the new state\n",
    "        current_state = next_state\n",
    "\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # print(f\"Episode {episode + 1 }/{EPISODES}:\")\n",
    "    TD_prediction_run_episode(world, equiprobable_random_policy, (0, 0))\n",
    "\n",
    "\n",
    "display(pd.DataFrame(V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK_bFSHsQJYr"
   },
   "source": [
    "## Exercise: SARSA\n",
    "\n",
    "Implement and test SARSA with an $\\epsilon$-greedy policy. See page 130 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) on different worlds. Make sure that it is easy to show a learnt policy (most probable action in each state). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wEIxyiw1J1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valdemar er dum:\n",
      "State: (0,0)\n",
      "         up : 4.25\n",
      "         down : 2.2\n",
      "         left : 4.63\n",
      "         right : 7.47\n",
      "\n",
      "\n",
      "State: (0,1)\n",
      "         up : 5.55\n",
      "         down : 0.01\n",
      "         left : 0.2\n",
      "         right : 0.15\n",
      "\n",
      "\n",
      "State: (0,2)\n",
      "         up : 0.4\n",
      "         down : 0.0\n",
      "         left : 0.0\n",
      "         right : 0.0\n",
      "\n",
      "\n",
      "State: (1,0)\n",
      "         up : 6.2\n",
      "         down : 8.87\n",
      "         left : 5.17\n",
      "         right : 4.98\n",
      "\n",
      "\n",
      "State: (1,1)\n",
      "         up : 5.67\n",
      "         down : 10.0\n",
      "         left : 2.19\n",
      "         right : 6.05\n",
      "\n",
      "\n",
      "State: (1,2)\n",
      "         up : 0.0\n",
      "         down : 0.0\n",
      "         left : 0.0\n",
      "         right : 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement your code here -- you need a Q-table to keep track of action \n",
    "#       value estimates and a policy-function that returns an epsilon greedy \n",
    "#       policy based on your estimates. \n",
    "\n",
    "epsilon = 0.1\n",
    "Q = np.full((world.width, world.height, 4), 0.0)\n",
    "alpha = 0.05\n",
    "\n",
    "\n",
    "def e_greedy(x,y):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(ACTIONS)\n",
    "        #return { k:1/len(ACTIONS) for k in ACTIONS }\n",
    "    else:\n",
    "        return ACTIONS[np.argmax(Q[(x,y)])]\n",
    "\n",
    "def sarsa(world, start_state):\n",
    "    current_state = start_state\n",
    "    current_action = e_greedy(*current_state)\n",
    "\n",
    "\n",
    "    while not world.is_terminal(*current_state):\n",
    "\n",
    "        # Get the next state from the world\n",
    "        next_state = world.get_next_state(current_state, current_action)\n",
    "        \n",
    "        # Get the reward for performing the action\n",
    "        reward = world.get_reward(*next_state)\n",
    "        \n",
    "        next_action = e_greedy(*next_state)\n",
    "\n",
    "        x, y = current_state\n",
    "        nx, ny = next_state\n",
    "\n",
    "        action_index = ACTIONS.index(current_action)\n",
    "        next_action_index = ACTIONS.index(next_action)\n",
    "        Q[x,y, action_index] = Q[x,y, action_index] + alpha * (reward + gamma * Q[nx,ny, next_action_index] - Q[x,y, action_index])\n",
    "\n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # print(f\"Episode {episode + 1 }/{EPISODES}:\")\n",
    "    sarsa(world, (0, 0))\n",
    "\n",
    "#Print Q\n",
    "print(\"Valdemar er dum:\")\n",
    "for x in range(world.width):\n",
    "    for y in range(world.height):\n",
    "        print(\"State:\", f\"({x},{y})\")\n",
    "        for i in range(len(ACTIONS)):\n",
    "            print(\"        \",ACTIONS[i],\":\",round(Q[(x,y)][i],2))\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHHfshQ8RtP-"
   },
   "source": [
    "## Exercise: Windy Gridworld\n",
    "\n",
    "Implement the Windy Gridworld (Example 6.5 on page 130 in the book) and test your SARSA implementation on the Windy Gridworld, first with the four actions (```up, down, left, right```) that move the agent in the cardinal directions, and then with King's moves as described in Exercise 6.9. How long does it take to learn a good policy for different values of $\\alpha$ and $\\epsilon$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RSPgN1YRI9dx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1      2      3      4      5      6      7      8     9\n",
      "0  right  right  right  right  right  right  right  right  right  down\n",
      "1  right     up  right  right  right     up  right  right  right  down\n",
      "2     up  right  right  right     up     up  right  right   down  down\n",
      "3  right  right  right  right  right  right  right   goal  right  down\n",
      "4  right   down  right  right  right  right     up   down   left  left\n",
      "5  right  right  right  right  right     up     up   down  right  down\n",
      "6  right     up  right  right     up     up     up     up     up  left\n",
      "|||||||||||||||||||||||KINGS MOVES|||||||||||||||||||\n",
      "        0       1       2       3       4       5       6       7       8  \\\n",
      "0  rightd      up   leftd   right   right  rightd   right  rightd  rightd   \n",
      "1   leftu  rightd  rightd  rightd   right   right   right   right  rightd   \n",
      "2  rightd  rightd   leftd  rightd   right  rightu  rightd  rightd  rightd   \n",
      "3  rightd  rightd  rightd   leftd  rightd  rightd  rightd    goal   leftd   \n",
      "4  rightd  rightd  rightd  rightd  rightd  rightd  rightd    down    left   \n",
      "5  rightd  rightd   right  rightd  rightd  rightd   right      up      up   \n",
      "6  rightu  rightu  rightu   right      up      up      up      up      up   \n",
      "\n",
      "       9  \n",
      "0   down  \n",
      "1   down  \n",
      "2  leftd  \n",
      "3  leftd  \n",
      "4   left  \n",
      "5     up  \n",
      "6  leftu  \n"
     ]
    }
   ],
   "source": [
    "### TODO: Implement and test SARSA, first on Windy Gridworld with four actions \n",
    "###       and then with King's moves\n",
    "\n",
    "\n",
    "wind_strength = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "wind_world = World(10, 7)\n",
    "wind_world.add_terminal(7, 3, \"+\")\n",
    "REWARDS = {\" \": -1, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "\n",
    "# Since we only focus on episodic tasks, we must have a terminal state that the \n",
    "# agent eventually reaches\n",
    "\n",
    "# print(wind_world.grid.T)\n",
    "\n",
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "Q = np.full((wind_world.width, wind_world.height, 4), 0.0)\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "\n",
    "def windsarsa(world, start_state):\n",
    "    current_state = start_state\n",
    "    current_action = e_greedy(*current_state)\n",
    "\n",
    "\n",
    "    while not world.is_terminal(*current_state):\n",
    "\n",
    "        # Get the next state from the world\n",
    "        next_state = world.get_next_state(current_state, current_action)\n",
    "\n",
    "        next_state = (next_state[0], next_state[1] - wind_strength[current_state[0]])\n",
    "\n",
    "        if (next_state[1] < 0):\n",
    "            next_state = (next_state[0], 0)\n",
    "        if (next_state[1] > world.height-1):\n",
    "            next_state = (next_state[0], world.height-1)\n",
    "        \n",
    "        # Get the reward for performing the action\n",
    "        reward = world.get_reward(*next_state)\n",
    "        \n",
    "        next_action = e_greedy(*next_state)\n",
    "\n",
    "\n",
    "\n",
    "        x, y = current_state\n",
    "        nx, ny = next_state\n",
    "\n",
    "        action_index = ACTIONS.index(current_action)\n",
    "        next_action_index = ACTIONS.index(next_action)\n",
    "        Q[x,y, action_index] = Q[x,y, action_index] + alpha * (reward + gamma * Q[nx,ny, next_action_index] - Q[x,y, action_index])\n",
    "\n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "\n",
    "EPISODES = 10000\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # print(f\"Episode {episode + 1 }/{EPISODES}:\")\n",
    "    windsarsa(wind_world, (0, 3))\n",
    "\n",
    "# print(Q)\n",
    "\n",
    "highest_action = np.full((wind_world.width, wind_world.height), '     ')\n",
    "for x in range(wind_world.width):\n",
    "    for y in range(wind_world.height):\n",
    "        highest_action[x, y] = ACTIONS[np.argmax(Q[x, y])]\n",
    "highest_action[7, 3] = \"goal\"\n",
    "print(pd.DataFrame(highest_action.T))\n",
    "print(\"|||||||||||||||||||||||KINGS MOVES|||||||||||||||||||\")\n",
    "###### KINGS MOVES\n",
    "epsilon = 0.5\n",
    "alpha = 0.05\n",
    "Q = np.full((wind_world.width, wind_world.height, 8), 0.0)\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\", \"rightu\", \"rightd\", \"leftu\", \"leftd\") \n",
    "EPISODES = 10000\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # print(f\"Episode {episode + 1 }/{EPISODES}:\")\n",
    "    windsarsa(wind_world, (0, 3))\n",
    "\n",
    "highest_action_king = np.full((wind_world.width, wind_world.height), '      ')\n",
    "for x in range(wind_world.width):\n",
    "    for y in range(wind_world.height):\n",
    "        highest_action_king[x, y] = ACTIONS[np.argmax(Q[x, y])]\n",
    "highest_action_king[7, 3] = \"goal\"\n",
    "print(pd.DataFrame(highest_action_king.T))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
