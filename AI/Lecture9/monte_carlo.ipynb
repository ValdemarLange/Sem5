{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMapQn0-y2Bk"
   },
   "source": [
    "# Monte Carlo approaches to prediction and control\n",
    "\n",
    "In this notebook, you will implement the Monte Carlo approaches to prediction and control described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lecture, but now without relying on knowledge of the task dynamics, that is, without relying on knowledge about transition probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9vApPmQFGUs"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9pkLaPNMOTP",
    "outputId": "e95b7cec-4f04-45df-b7b4-18963e4a6b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: pandas in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8kdEPNmFOCr"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T9cAvA0GLkXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys          # We use sys to get the max value of a float\n",
    "import pandas as pd # We only use pandas for displaying tables nicely\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTNglEH9FR8f"
   },
   "source": [
    "### ```World``` class and globals\n",
    "\n",
    "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called.\n",
    "\n",
    "__Note that ```get_state_transition_probabilities``` has been removed and an agent must now rely on experience interacting with a world to learn.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wMAd6qTASn9u"
   },
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\") # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:\n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    # Create an empty world where the agent can move to all cells\n",
    "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
    "\n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "\n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def add_terminal(self, x, y, terminal):\n",
    "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "    self.grid[x, y] = terminal\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES\n",
    "\n",
    "  def is_terminal(self, x, y):\n",
    "    return self.grid[x ,y] in TERMINALS\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\"\n",
    "    Return the reward associated with a given location\n",
    "    \"\"\"\n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. The outcome can be\n",
    "    stochastic  where rand_move_probability determines the probability of\n",
    "    ignoring the action and performing a random move.\n",
    "    \"\"\"\n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "    x, y = current_state\n",
    "\n",
    "    # If our current state is a terminal, there is no next state\n",
    "    if self.grid[x, y] in TERMINALS:\n",
    "      return None\n",
    "\n",
    "    # Check of a random action should be performed:\n",
    "    if np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    # If the next state is an obstacle, stay in the current state\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxncUHvz1_Hb"
   },
   "source": [
    "## Basic example: Generating episodes\n",
    "\n",
    "An episode is the series of states, actions and rewards reflecting an agent's experience interacting with the environment. An episode starts with an agent being placed at some initial state and continues till the agent reaches a terminal state.  To generate episodes, we first need a world and a policy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dL-XC6-aN6ss",
    "outputId": "03a82fc1-d2b9-4128-de6a-a2a7fc67551e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
      "[[' ' ' ']\n",
      " [' ' ' ']\n",
      " [' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "world = World(2, 3)\n",
    "\n",
    "# Since we only focus on episodic tasks, we must have a terminal state that the\n",
    "# agent eventually reaches\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "\n",
    "def equiprobable_random_policy(x, y):\n",
    "  return { k:1/len(ACTIONS) for k in ACTIONS }\n",
    "\n",
    "print(equiprobable_random_policy(1,1))\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfMg_Zjl40vx"
   },
   "source": [
    "To generate an episode, we need to provide a ```World```, a policy, and a start state.\n",
    "\n",
    "In each step, we do the following:\n",
    "1. perform one of the actions (weighted random) returned by the policy for the giving state\n",
    "2. get the reward and add a new entry to the episode $[S_t, A_t, R_{t+1}]$\n",
    "3. move the agent to the next state\n",
    "\n",
    "When a terminal state is reached, we return all the $[[S_0, A_0, R_1], ..., [S_{T}, A_T, R_{T+1}]]$ observed in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "51rP-8eH4w1V"
   },
   "outputs": [],
   "source": [
    "def generate_episode(world, policy, start_state):\n",
    "    current_state = start_state\n",
    "    episode = []\n",
    "    while not world.is_terminal(*current_state):\n",
    "        # Get the possible actions and their probabilities that our policy says\n",
    "        # that the agent should perform in the current state:\n",
    "        possible_actions = policy(*current_state)\n",
    "\n",
    "        # Pick a weighted random action:\n",
    "        action = random.choices(population=list(possible_actions.keys()),\n",
    "                                weights=possible_actions.values(), k=1)\n",
    "\n",
    "        # Get the next state from the world\n",
    "        next_state = world.get_next_state(current_state, action[0])\n",
    "\n",
    "        # Get the reward for performing the action\n",
    "        reward = world.get_reward(*next_state)\n",
    "\n",
    "        # Save the state, action and reward for this time step in our episode\n",
    "        episode.append([current_state, action[0], reward])\n",
    "\n",
    "        # Move the agent to the new state\n",
    "        current_state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTNH2PP06lBJ"
   },
   "source": [
    "Now, we can try to generate a couple of episodes and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PsyPGgMt2skf",
    "outputId": "c121ddf6-479e-4317-8c69-3c093ded5a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0:\n",
      "    State Action  Reward\n",
      "0  (0, 0)  right       0\n",
      "1  (1, 0)     up       0\n",
      "2  (1, 0)   down       0\n",
      "3  (1, 1)   down      10\n",
      "\n",
      "Episode 1:\n",
      "     State Action  Reward\n",
      "0   (0, 0)   left       0\n",
      "1   (0, 0)     up       0\n",
      "2   (0, 0)  right       0\n",
      "3   (1, 0)   down       0\n",
      "4   (1, 1)  right       0\n",
      "5   (1, 1)   left       0\n",
      "6   (0, 1)   down       0\n",
      "7   (0, 2)     up       0\n",
      "8   (0, 1)     up       0\n",
      "9   (0, 0)   left       0\n",
      "10  (0, 0)   down       0\n",
      "11  (0, 1)  right       0\n",
      "12  (1, 1)     up       0\n",
      "13  (1, 0)   left       0\n",
      "14  (0, 0)   down       0\n",
      "15  (0, 1)     up       0\n",
      "16  (0, 0)  right       0\n",
      "17  (1, 0)   left       0\n",
      "18  (0, 0)  right       0\n",
      "19  (1, 0)     up       0\n",
      "20  (1, 0)     up       0\n",
      "21  (1, 0)   down       0\n",
      "22  (1, 1)   down      10\n",
      "\n",
      "Episode 2:\n",
      "     State Action  Reward\n",
      "0   (0, 0)  right       0\n",
      "1   (1, 0)   down       0\n",
      "2   (1, 1)   left       0\n",
      "3   (0, 1)     up       0\n",
      "4   (0, 0)  right       0\n",
      "5   (1, 0)  right       0\n",
      "6   (1, 0)   down       0\n",
      "7   (1, 1)   left       0\n",
      "8   (0, 1)  right       0\n",
      "9   (1, 1)  right       0\n",
      "10  (1, 1)     up       0\n",
      "11  (1, 0)  right       0\n",
      "12  (1, 0)     up       0\n",
      "13  (1, 0)   left       0\n",
      "14  (0, 0)     up       0\n",
      "15  (0, 0)  right       0\n",
      "16  (1, 0)   left       0\n",
      "17  (0, 0)   left       0\n",
      "18  (0, 0)   left       0\n",
      "19  (0, 0)  right       0\n",
      "20  (1, 0)   left       0\n",
      "21  (0, 0)   left       0\n",
      "22  (0, 0)   left       0\n",
      "23  (0, 0)   left       0\n",
      "24  (0, 0)     up       0\n",
      "25  (0, 0)     up       0\n",
      "26  (0, 0)     up       0\n",
      "27  (0, 0)   down       0\n",
      "28  (0, 1)   left       0\n",
      "29  (0, 1)  right       0\n",
      "30  (1, 1)   down      10\n",
      "\n",
      "Episode 3:\n",
      "     State Action  Reward\n",
      "0   (0, 0)     up       0\n",
      "1   (0, 0)   left       0\n",
      "2   (0, 0)     up       0\n",
      "3   (0, 0)   down       0\n",
      "4   (0, 1)   down       0\n",
      "5   (0, 2)   down       0\n",
      "6   (0, 2)     up       0\n",
      "7   (0, 1)   down       0\n",
      "8   (0, 2)   down       0\n",
      "9   (0, 2)   down       0\n",
      "10  (0, 2)  right      10\n",
      "\n",
      "Episode 4:\n",
      "     State Action  Reward\n",
      "0   (0, 0)   left       0\n",
      "1   (0, 0)   down       0\n",
      "2   (0, 1)     up       0\n",
      "3   (0, 0)   down       0\n",
      "4   (0, 1)   left       0\n",
      "5   (0, 1)   down       0\n",
      "6   (0, 2)     up       0\n",
      "7   (0, 1)   left       0\n",
      "8   (0, 1)   left       0\n",
      "9   (0, 1)   down       0\n",
      "10  (0, 2)     up       0\n",
      "11  (0, 1)  right       0\n",
      "12  (1, 1)   left       0\n",
      "13  (0, 1)   left       0\n",
      "14  (0, 1)  right       0\n",
      "15  (1, 1)  right       0\n",
      "16  (1, 1)  right       0\n",
      "17  (1, 1)  right       0\n",
      "18  (1, 1)     up       0\n",
      "19  (1, 0)     up       0\n",
      "20  (1, 0)     up       0\n",
      "21  (1, 0)  right       0\n",
      "22  (1, 0)   down       0\n",
      "23  (1, 1)   down      10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Episode {i}:\")\n",
    "    episode = generate_episode(world, equiprobable_random_policy, (0, 0))\n",
    "    print(pd.DataFrame(episode, columns=[\"State\", \"Action\", \"Reward\"]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLj3TWX3xDv-"
   },
   "source": [
    "### Exercise: Implement Monte Carlo-based prediction for state values\n",
    "\n",
    "You should implement first-visit MC prediction for estimating $V≈v_\\pi$. See page 92 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LuBuwnmZy8Dg"
   },
   "outputs": [],
   "source": [
    "### TODO: Implement your code here\n",
    "def first_visit_MC(world, policy):\n",
    "    V = np.full((world.width, world.height), 0.0)\n",
    "    sum_of_returns = np.full((world.width, world.height), 0.0)\n",
    "    times_visited = np.full((world.width, world.height), 0.0)\n",
    "\n",
    "\n",
    "    for _ in range(5000):\n",
    "        episode = generate_episode(world, policy, (np.random.randint(world.width-1), np.random.randint(world.height-1)))\n",
    "\n",
    "        G = 0\n",
    "\n",
    "        # loop for each step in episode\n",
    "        for i in reversed(range(len(episode))): #loop t=T-1, T-2,..,0\n",
    "            state, action, reward = episode[i]\n",
    "            G = gamma * G + reward\n",
    "            isVisited = False\n",
    "            for j in range(i-1):    # Tjek om det first visit\n",
    "                previous_state = episode[j][0]\n",
    "                if state == previous_state: \n",
    "                    isVisited = True\n",
    "                    break\n",
    "\n",
    "            if not isVisited:       # Ellers \"append G\" og returner avg G\n",
    "                sum_of_returns[state] += G\n",
    "                times_visited[state] += 1\n",
    "                V[state] = sum_of_returns[state] / times_visited[state]\n",
    "        \n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSgpTNFBy8eq"
   },
   "source": [
    "First, try your algorithm on the small $2\\times3$ world above using an equiprobable policy and $\\gamma = 0.9$. Depending on the number of episodes you use, you should get close to the true values:\n",
    "\n",
    "<table class=\"dataframe\" border=\"1\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>0</th>\n",
    "      <th>1</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>3.283</td>\n",
    "      <td>3.616</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>4.409</td>\n",
    "      <td>5.556</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>6.349</td>\n",
    "      <td>0.000</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eRvNY8oR6tjs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1\n",
      "0 3.225 3.552\n",
      "1 4.408 5.529\n",
      "2 6.289 0.000\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "### TODO: Implement your code here\n",
    "V1 = first_visit_MC(world, equiprobable_random_policy)\n",
    "\n",
    "print(pd.DataFrame(V1).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AxMw0aZ2I-9"
   },
   "source": [
    "Try to run your MC prediction code on worlds of different sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long). You can try to change the policy as well, but rememeber that the agent **must** eventually reach a terminal state under any policy that you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yjORC8Zl2JWl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' '+' ' ' ' ']\n",
      " [' ' ' ' ' ' '+']]\n",
      "      0     1     2     3\n",
      "0 2.842 3.092 2.801 2.742\n",
      "1 4.009 4.971 4.033 3.851\n",
      "2 5.805 0.000 6.336 6.256\n",
      "3 4.972 6.551 6.971 0.000\n"
     ]
    }
   ],
   "source": [
    "### TODO: Implement your code here\n",
    "world2 = World(4, 4)\n",
    "\n",
    "# Since we only focus on episodic tasks, we must have a terminal state that the\n",
    "# agent eventually reaches\n",
    "world2.add_terminal(1, 2, \"+\")\n",
    "world2.add_terminal(3, 3, \"+\")\n",
    "\n",
    "print(world2.grid.T)\n",
    "\n",
    "V2 = first_visit_MC(world2, equiprobable_random_policy)\n",
    "print(pd.DataFrame(V2).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsCCnIH4z_g1"
   },
   "source": [
    "### Exercise: Implement Monte Carlo-based prediction for state-action values\n",
    "\n",
    "There is one more step that has to be in place before we can start to optimize a policy: estimating state-action values, $q_\\pi(s,a)$, based on experience. Above where we estimated $v_\\pi$, we only needed to keep track of the average return observed for _each state_. However, in order to estimate state-action values, we need to compute the average return observed for _each state-action_ pair.\n",
    "\n",
    "That is, for every state $(0,0), (0,1), (0,2)...$ we need to compute different estimates for the four actions ```[ \"up\", \"down\", \"left\", \"right\" ]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5ejjYJ65sQKW"
   },
   "outputs": [],
   "source": [
    "### TODO: Implement your code here to predict state-action values.\n",
    "def exploring_starts_MC(world):\n",
    "    pol = np.full((world.width, world.height), None)\n",
    "    Q = np.full((world.width, world.height, 4), 0.0)\n",
    "    sum_of_returns = np.full((world.width, world.height, 4), 0.0)\n",
    "    times_visited = np.full((world.width, world.height, 4), 0.0)\n",
    "\n",
    "    action_to_index = {action: idx for idx, action in enumerate(ACTIONS)}\n",
    "\n",
    "    for _ in range(5000):\n",
    "        episode = generate_episode(world, equiprobable_random_policy, (np.random.randint(world.width-1), np.random.randint(world.height-1)))\n",
    "        G = 0\n",
    "\n",
    "        for i in reversed(range(len(episode))):\n",
    "            G = gamma * G + episode[i][2]\n",
    "            isVisited = False\n",
    "            for j in range(i-1):\n",
    "                if episode[i][0] == episode[j][0] and episode[i][1] == episode[j][1]:\n",
    "                    isVisited = True\n",
    "                    break\n",
    "\n",
    "            if not isVisited:\n",
    "                this_action = action_to_index[episode[i][1]]\n",
    "                x, y = episode[i][0]\n",
    "                sum_of_returns[x, y, this_action] += G\n",
    "                times_visited[x, y, this_action] += 1\n",
    "                Q[x, y, this_action] = sum_of_returns[x, y, this_action] / times_visited[x, y, this_action]\n",
    "                \n",
    "                pol[x, y] = ACTIONS[np.argmax(Q[x, y])]\n",
    "        \n",
    "\n",
    "    return pol       \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4qrvNKClqpc"
   },
   "source": [
    "Try to experiment with your implementation by running it on different world sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long), and try to experiment with different numbers of episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E1f_Om_aOmVj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['down' 'down']\n",
      " ['down' 'down']\n",
      " ['right' None]]\n"
     ]
    }
   ],
   "source": [
    "### TODO: Implement your code here\n",
    "pol = exploring_starts_MC(world)\n",
    "    \n",
    "\n",
    "print(pol.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fU4gAj-OThu9"
   },
   "source": [
    "### Exercise: Implement on-policy Monte Carlo-based control with an $\\epsilon$-soft policy\n",
    "\n",
    "You are now ready to implement MC-based control (see page 101 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the algorithm).\n",
    "\n",
    "In your implementation, you need to update the state-action estimates like in the exercise above, but now, you also need implement an $ϵ$-soft policy that you can modify. How could you do that?\n",
    "\n",
    "_Hint_: You can either represent your policy explicitly. That is, for each state $(x,y)$ you have a ```dict``` with actions and their probabilities which you then update each time you step through an episode. When the policy is called, it then just returns the ```dict``` with action probablities corresponding to the current state.\n",
    "\n",
    "Alternatively, you can compute the action probabilities when your policy is called based on the current action-values estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fYJpKFy82PId"
   },
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "\n",
    "sum_of_returns_soft = np.full((world.width, world.height, 4), 0.0)\n",
    "times_visited_soft = np.full((world.width, world.height, 4), 1.0)\n",
    "\n",
    "def MC_soft_policy(x,y):\n",
    "    Q = []\n",
    "    for acts in ACTIONS:\n",
    "        Q.append(sum_of_returns_soft[x,y,ACTIONS.index(acts)] / times_visited_soft[x,y,ACTIONS.index(acts)])\n",
    "    \n",
    "    best_action = ACTIONS[np.argmax(Q)]\n",
    "    # print(best_action)\n",
    "\n",
    "    if np.random.rand() < 1 - epsilon:\n",
    "        return {best_action:1}\n",
    "    else:\n",
    "        return { k:1/len(ACTIONS) for k in ACTIONS }\n",
    "        \n",
    "\n",
    "MC_soft_policy(1,1)\n",
    "\n",
    "def reset_soft(world):\n",
    "    global sum_of_returns_soft, times_visited_soft\n",
    "    sum_of_returns_soft = np.full((world.width, world.height, 4), 0.0)\n",
    "    times_visited_soft = np.full((world.width, world.height, 4), 1.0)\n",
    "\n",
    "\n",
    "### TODO: Implement you code here. You need to define a policy function\n",
    "###       and then the actual algorithm that goes through time step in each\n",
    "###       episode and updates state-action values and the policy. Also, make\n",
    "###       sure that you can print out the policy learned, that is, the action\n",
    "###       with the highest expected value in each state.\n",
    "def soft_MC(world):\n",
    "    pol = np.full((world.width, world.height), None)\n",
    "    Q = np.full((world.width, world.height, 4), 0.0)\n",
    "\n",
    "    action_to_index = {action: idx for idx, action in enumerate(ACTIONS)}\n",
    "\n",
    "    for _ in range(5000):\n",
    "        episode = generate_episode(world, MC_soft_policy, (np.random.randint(world.width-1), np.random.randint(world.height-1)))\n",
    "        G = 0\n",
    "\n",
    "        for i in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[i]\n",
    "            G = gamma * G + reward\n",
    "            isVisited = False\n",
    "            for j in range(i-1):\n",
    "                previous_state, previous_action, previous_reward = episode[j]\n",
    "                if state == previous_state and action == previous_action:\n",
    "                    isVisited = True\n",
    "                    break\n",
    "\n",
    "            if not isVisited:\n",
    "                this_action = action_to_index[action]\n",
    "                x, y = state\n",
    "                sum_of_returns_soft[x, y, this_action] += G\n",
    "                times_visited_soft[x, y, this_action] += 1\n",
    "                Q[x, y, this_action] = sum_of_returns_soft[x, y, this_action] / times_visited_soft[x, y, this_action]\n",
    "                \n",
    "                pol[x, y] = ACTIONS[np.argmax(Q[x, y])]\n",
    "        \n",
    "\n",
    "    return pol      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7KVqyXJnMo6"
   },
   "source": [
    "Try to experiment with your implementation by running it on different world sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long), try to experiment with different numbers of episodes, and different values of epsilon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ppZT2RlwcsXM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0      \n",
       "1      \n",
       "2     +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1\n",
       "0   down  down\n",
       "1  right  down\n",
       "2  right  None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0            \n",
       "1            \n",
       "2     +      \n",
       "3           +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>None</td>\n",
       "      <td>left</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3\n",
       "0  right  down  down  down\n",
       "1  right  down  down  left\n",
       "2  right  None  left    up\n",
       "3     up    up    up  None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0            \n",
       "1            \n",
       "2     +      \n",
       "3           +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "      <td>left</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>None</td>\n",
       "      <td>left</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>right</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1      2     3\n",
       "0   down  down   down  down\n",
       "1  right  down   left  down\n",
       "2  right  None   left  down\n",
       "3     up    up  right  None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### TODO: Implement your code here\n",
    "epsilon = 0.1\n",
    "\n",
    "pol1 = soft_MC(world)\n",
    "\n",
    "#print(pol1.T)\n",
    "print(\"epsilon = 0.1\")\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "display(pd.DataFrame(pol1.T))\n",
    "\n",
    "reset_soft(world2)\n",
    "pol2 = soft_MC(world2)\n",
    "\n",
    "#print(pol2.T)\n",
    "print(\"epsilon = 0.1\")\n",
    "display(pd.DataFrame(world2.grid.T))\n",
    "display(pd.DataFrame(pol2.T))\n",
    "\n",
    "reset_soft(world2)\n",
    "epsilon = 0.3 # Den her finder oftere den bedste løsning (ses ved 3.kolonne = ned og (2,3) = right)\n",
    "pol3 = soft_MC(world2)\n",
    "#print(pol3.T)\n",
    "print(\"epsilon = 0.3\")\n",
    "display(pd.DataFrame(world2.grid.T))\n",
    "display(pd.DataFrame(pol3.T))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdgcLeDnZre"
   },
   "source": [
    "### Optional exercise\n",
    "\n",
    "Try to implement exploring starts (see page 99 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the algorithm). It should be straightforward and only require minimal changes to the code for the exercise above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JEG IKKE SIKKER PÅ AT JEG HAR LAVET DET OPGAVEN FAKTISK SPURGTE OM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_returns_soft_exp = np.full((world.width, world.height, 4), 0.0)\n",
    "times_visited_soft_exp = np.full((world.width, world.height, 4), 1.0)\n",
    "isStart = True\n",
    "\n",
    "def MC_soft_exploring_policy(x,y):\n",
    "    global isStart\n",
    "    Q = []\n",
    "    for acts in ACTIONS:\n",
    "        Q.append(sum_of_returns_soft_exp[x,y,ACTIONS.index(acts)] / times_visited_soft_exp[x,y,ACTIONS.index(acts)])\n",
    "    \n",
    "    best_action = ACTIONS[np.argmax(Q)]\n",
    "    # print(best_action)\n",
    "    if isStart:\n",
    "        return { k:1/len(ACTIONS) for k in ACTIONS }\n",
    "\n",
    "    if np.random.rand() < 1 - epsilon or not isStart:\n",
    "        return {best_action:1}\n",
    "    else:\n",
    "        isStart = False\n",
    "        return { k:1/len(ACTIONS) for k in ACTIONS }\n",
    "\n",
    "\n",
    "def reset_soft_exploring(world):\n",
    "    global sum_of_returns_soft_exp, times_visited_soft_exp, isStart\n",
    "    sum_of_returns_soft_exp = np.full((world.width, world.height, 4), 0.0)\n",
    "    times_visited_soft_exp = np.full((world.width, world.height, 4), 1.0)\n",
    "    isStart = True\n",
    "\n",
    "def soft_exploring_MC(world):\n",
    "    global isStart\n",
    "    pol = np.full((world.width, world.height), None)\n",
    "    Q = np.full((world.width, world.height, 4), 0.0)\n",
    "\n",
    "    action_to_index = {action: idx for idx, action in enumerate(ACTIONS)}\n",
    "\n",
    "    for _ in range(5000):\n",
    "        \n",
    "        episode = generate_episode(world, MC_soft_exploring_policy, (np.random.randint(world.width-1), np.random.randint(world.height-1)))\n",
    "        G = 0\n",
    "\n",
    "        for i in reversed(range(len(episode))):\n",
    "            G = gamma * G + episode[i][2]\n",
    "            isVisited = False\n",
    "            for j in range(i-1):\n",
    "                if episode[i][0] == episode[j][0] and episode[i][1] == episode[j][1]:\n",
    "                    isVisited = True\n",
    "                    break\n",
    "\n",
    "            if not isVisited:\n",
    "                this_action = action_to_index[episode[i][1]]\n",
    "                x, y = episode[i][0]\n",
    "                sum_of_returns_soft_exp[x, y, this_action] += G\n",
    "                times_visited_soft_exp[x, y, this_action] += 1\n",
    "                Q[x, y, this_action] = sum_of_returns_soft_exp[x, y, this_action] / times_visited_soft_exp[x, y, this_action]\n",
    "                \n",
    "                pol[x, y] = ACTIONS[np.argmax(Q[x, y])]\n",
    "        \n",
    "        isStart = True\n",
    "        \n",
    "\n",
    "    return pol      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['down' 'down']\n",
      " ['down' 'down']\n",
      " ['right' None]]\n",
      "[['down' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['right' None 'left' 'down']\n",
      " ['right' 'up' 'right' None]]\n",
      "[['down' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['right' None 'left' 'down']\n",
      " ['right' 'up' 'right' None]]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "# Selvom antal episoder er sat ned til 5000 (fordi den her åbenbart tager meget længere tid), \n",
    "# så finder den stadig nogle gode policys fordi den er mere tilbøjelig til at udforske på første forsøg \n",
    "# fx så var (2,3) med eps = 0.1 ikke altid udforsket nok til at vide der var en terminal state lige til højre for den.\n",
    "# Men her hvor den altid udforsker første action så er der en 1/64 del chance for at den finder netop den sti. \n",
    "#      16 felter og 4 actions => 1/16 * 1/4 = 1/64 del chance for lige netop det felt og action\n",
    "\n",
    "reset_soft_exploring(world2)\n",
    "pol4 = soft_exploring_MC(world)\n",
    "\n",
    "print(pol4.T)\n",
    "\n",
    "reset_soft_exploring(world2)\n",
    "pol5 = soft_exploring_MC(world2)\n",
    "\n",
    "print(pol5.T)\n",
    "\n",
    "reset_soft_exploring(world2)\n",
    "epsilon = 0.3 \n",
    "pol6 = soft_exploring_MC(world2)\n",
    "print(pol6.T)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
