{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMapQn0-y2Bk"
   },
   "source": [
    "# Expected SARSA and $n$-step bootstrapping \n",
    "\n",
    "In this notebook, you will implement expected SARSA and $n$-step bookstrapping described in Chapters 6 and 7 of [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9vApPmQFGUs"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e9pkLaPNMOTP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: pandas in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/valdemar/GitHub/Sem5/AI/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8kdEPNmFOCr"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T9cAvA0GLkXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys          # We use sys to get the max value of a float\n",
    "import pandas as pd # We only use pandas for displaying tables nicely\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTNglEH9FR8f"
   },
   "source": [
    "### ```World``` class and globals\n",
    "\n",
    "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wMAd6qTASn9u"
   },
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\") \n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\") # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:  \n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    # Create an empty world where the agent can move to all cells\n",
    "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
    "  \n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "    \n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def add_terminal(self, x, y, terminal):\n",
    "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "    self.grid[x, y] = terminal\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES \n",
    "\n",
    "  def is_terminal(self, x, y):\n",
    "    return self.grid[x ,y] in TERMINALS\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\" \n",
    "    Return the reward associated with a given location\n",
    "    \"\"\" \n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. The outcome can be\n",
    "    stochastic  where rand_move_probability determines the probability of \n",
    "    ignoring the action and performing a random move.\n",
    "    \"\"\"    \n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "    \n",
    "    x, y = current_state \n",
    "    \n",
    "    # If our current state is a terminal, there is no next state\n",
    "    if self.grid[x, y] in TERMINALS:\n",
    "      return None\n",
    "\n",
    "    # Check of a random action should be performed:\n",
    "    if np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    # If the next state is an obstacle, stay in the current state\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKybFbwCG478"
   },
   "source": [
    "## A simple world\n",
    "\n",
    "We will create a simple $8\\times8$ world, where the agent should start in the top-left corner at $(0,0)$ and find its way to the bottom-right corner at $(7,7)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVpq3vosHAol",
    "outputId": "c29365e2-2dea-4772-907d-af80719ffa37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "world = World(8, 8) # 8 8\n",
    "world.add_terminal(7, 7, \"+\") # 7 7\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK_bFSHsQJYr"
   },
   "source": [
    "## Exercise: Expected SARSA\n",
    "\n",
    "Implement and test expected SARSA, see page 133 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). The implementation of expected SARSA is very similar to regular SARSA, so you can use your SARSA implementation as a starting point for your expected SARSA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wEIxyiw1J1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1      2      3      4      5      6      7\n",
      "0   down   down   left     up   down   left     up   down\n",
      "1   down   down  right   left   left   left   left   down\n",
      "2   down   down   down   left   left   left     up     up\n",
      "3   down  right   down     up     up     up   down     up\n",
      "4   down   down   left   left   left     up   down  right\n",
      "5  right   down     up   down   down   left     up   left\n",
      "6     up  right   down   down   left   down   down     up\n",
      "7  right  right  right  right  right  right  right   GOAL\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement your code here -- you need a Q-table to keep track of action \n",
    "#       value estimates and a policy-function that returns an epsilon greedy \n",
    "#       policy based on your estimates.\n",
    "\n",
    "epsilon = 0.1\n",
    "Q = np.random.rand(world.width, world.height, len(ACTIONS))\n",
    "gamma = 0.9\n",
    "alpha = 0.05\n",
    "\n",
    "\n",
    "def SARSA_policy(x,y):\n",
    "    actions = { a : epsilon/len(ACTIONS) for a in ACTIONS }\n",
    "    actions[ACTIONS[np.argmax(Q[x,y,:])]] = 1 - epsilon + epsilon/len(ACTIONS)\n",
    "    return actions\n",
    "\n",
    "def expected_sarsa(world, policy, start_state):\n",
    "    steps = 0\n",
    "    current_state = start_state\n",
    "\n",
    "    possible_actions = policy(*current_state)\n",
    "    current_action = random.choices(population=list(possible_actions.keys()), weights=possible_actions.values(), k=1)\n",
    "    # print(current_action)\n",
    "\n",
    "\n",
    "    while not world.is_terminal(*current_state):\n",
    "        steps += 1\n",
    "        # print(steps)\n",
    "        # Get the next state from the world\n",
    "        next_state = world.get_next_state(current_state, current_action[0])\n",
    "\n",
    "        possible_actions = policy(*next_state)\n",
    "        \n",
    "        # Get the reward for performing the action\n",
    "        reward = world.get_reward(*next_state)\n",
    "        \n",
    "        next_action = random.choices(population=list(possible_actions.keys()), weights=possible_actions.values(), k=1)\n",
    "\n",
    "\n",
    "        x, y = current_state\n",
    "        nx, ny = next_state\n",
    "\n",
    "        expected_value = 0\n",
    "        for action in ACTIONS:\n",
    "            action_prob = possible_actions[action]\n",
    "            action_index = ACTIONS.index(action)\n",
    "            expected_value += action_prob + Q[nx, ny, action_index]\n",
    "\n",
    "        Q[x,y, ACTIONS.index(current_action[0])] = Q[x,y, ACTIONS.index(current_action[0])] + alpha * (reward + gamma * Q[nx,ny, ACTIONS.index(next_action[0])] - Q[x,y, ACTIONS.index(current_action[0])])\n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # print(f\"Episode {episode + 1 }/{EPISODES}:\")\n",
    "    expected_sarsa(world, SARSA_policy, (0, 0))\n",
    "    \n",
    "highest_action = np.full((world.width, world.height), '      ')\n",
    "for x in range(world.width):\n",
    "    for y in range(world.height):\n",
    "        highest_action[x, y] = ACTIONS[np.argmax(Q[x, y])]\n",
    "highest_action[7, 7] = 'GOAL'\n",
    "print(pd.DataFrame(highest_action.T))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2WCoz60zN_t"
   },
   "source": [
    "Try your expected SARSA implementation on the $8\\times8$ world defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_J7MOXnFgXZI"
   },
   "outputs": [],
   "source": [
    "### TODO: try your expected SARSA implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPwkIEt_hRlS"
   },
   "source": [
    "### Test and compare your expected SARSA implementation\n",
    "\n",
    "Test your implementation on the simple $8 \\times 8$ world defined above with, ```rand_move_probability = 0.0```, $\\epsilon = 0.1$, and $\\gamma = 0.9$. Compare the performance of expected SARSA with normal SARSA. How long does it take for the respective methods to solve the tasks optimally (that is, number of steps that an agent takes from start to finish = _width_ + _height_ $-2 = 14$). \n",
    "\n",
    "Remember that for expected SARSA, you can use a learning rate of 1 ($\\alpha = 1)$, whereas for SARSA, you should try different learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Dz1Sqh95hoyC"
   },
   "outputs": [],
   "source": [
    "rand_move_probability = 0.0\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "### TODO: Compare the performance of SARSA (different alphas) and \n",
    "###       expected SARSA (alpha = 1). You need to run multiple \n",
    "###       experiments (e.g. 100 per setting) and then take the average \n",
    "###       to get a proper estimate of the general performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwPpd9OKmqYb"
   },
   "source": [
    "## Exercise: $n$-step on-policy SARSA or $n$-step on-policy expected SARSA\n",
    "\n",
    "Here, you should implement on-policy $n$-step bootstrapping. You can either implement $n$-step SARSA or $n$-step expected SARSA. See Chapter 7 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) and page 147 for $n$-step SARSA.\n",
    "\n",
    "Test you implementation with different values of $n$ either on the $8\\times8$ world above or on the ```WindyGridWorld``` from previous lectures. It is up to you to decide how to measure performance: it could, for instance, be the average episode length after a limited number of episodes (for instance $10$), how long it takes to solve the task optimally, like in the exercise on expected SARSA above, or something else.\n",
    "\n",
    "For the world that you choose, you have to answer the question: \"What is a good value for $n$?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "MsdFod-Xx5iY"
   },
   "outputs": [],
   "source": [
    "### TODO: Implment and test n-step SARSA or n-step expected SARSA.\n",
    "\n",
    "epsilon = 0.1\n",
    "Q = np.random.rand(world.width, world.height, len(ACTIONS))\n",
    "gamma = 0.9\n",
    "alpha = 0.05\n",
    "\n",
    "def e_greedy(x,y):\n",
    "    actions = { a : epsilon/len(ACTIONS) for a in ACTIONS }\n",
    "    actions[ACTIONS[np.argmax(Q[x,y,:])]] = 1 - epsilon + epsilon/len(ACTIONS)\n",
    "    return actions\n",
    "\n",
    "def nsarsa(world, n, start_state):\n",
    "    state_buffer = [None] * (n + 1) ## Buffer til at gemme state, action og rewards\n",
    "    action_buffer = [None] * (n + 1)\n",
    "    reward_buffer = [None] * (n + 1)\n",
    "\n",
    "    current_state = start_state\n",
    "    possible_actions = e_greedy(*current_state)\n",
    "    current_action = random.choices(\n",
    "        population=list(possible_actions.keys()),\n",
    "        weights=possible_actions.values(),\n",
    "        k=1\n",
    "    )\n",
    "    current_action_no_list = current_action[0]\n",
    "\n",
    "    T = np.inf\n",
    "    t = 0\n",
    "\n",
    "    while not world.is_terminal(*current_state) or t < T - 1:\n",
    "\n",
    "        if t < T:\n",
    "            # Get the next state from the world\n",
    "            next_state = world.get_next_state(current_state, current_action_no_list)\n",
    "\n",
    "            # Get the reward for performing the action\n",
    "            reward = world.get_reward(*next_state)\n",
    "\n",
    "            # Gem data i buffer\n",
    "            state_buffer[t % (n + 1)] = current_state # modulo laver en cirkulær datastruktur så gammel data overskrives\n",
    "            action_buffer[t % (n + 1)] = current_action\n",
    "            reward_buffer[t % (n + 1)] = reward\n",
    "            \n",
    "            if world.is_terminal(*next_state):\n",
    "                T = t + 1\n",
    "            else:\n",
    "                next_action = e_greedy(*next_state)\n",
    "\n",
    "                current_state = next_state\n",
    "                current_action = next_action\n",
    "\n",
    "        tau = t - n + 1\n",
    "\n",
    "        if tau >= 0:\n",
    "            G = np.sum(gamma**(i - tau -1) * reward_buffer[i % (n+1)] for i in range(tau, min(tau + n, T)))\n",
    "\n",
    "            if tau + n < T:\n",
    "                G += gamma**n * Q[state_buffer[(tau + n) % (n +1)], action_buffer[(tau + n) % (n + 1)]]\n",
    "                    \n",
    "            state_tau = state_buffer[tau % (n + 1)]\n",
    "            state_tau = tuple(state_tau)\n",
    "            action_tau = ACTIONS.index(action_buffer[tau % (n + 1)][0])\n",
    "          \n",
    "            Q[state_tau, action_tau] += alpha * (G - Q[state_tau, action_tau])\n",
    "\n",
    "        t += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8847/310318026.py:55: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  G = np.sum(gamma**(i - tau -1) * reward_buffer[i % (n+1)] for i in range(tau, min(tau + n, T)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,1,8,8,4) (2,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# print(f\"Episode {episode + 1 }/{EPISODES}:\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mnsarsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m highest_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull((world\u001b[38;5;241m.\u001b[39mwidth, world\u001b[38;5;241m.\u001b[39mheight), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m      \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(world\u001b[38;5;241m.\u001b[39mwidth):\n",
      "Cell \u001b[0;32mIn[51], line 64\u001b[0m, in \u001b[0;36mnsarsa\u001b[0;34m(world, n, start_state)\u001b[0m\n\u001b[1;32m     61\u001b[0m     state_tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(state_tau)\n\u001b[1;32m     62\u001b[0m     action_tau \u001b[38;5;241m=\u001b[39m ACTIONS\u001b[38;5;241m.\u001b[39mindex(action_buffer[tau \u001b[38;5;241m%\u001b[39m (n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 64\u001b[0m     Q[state_tau, action_tau] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m (\u001b[43mG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate_tau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_tau\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     66\u001b[0m t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,1,8,8,4) (2,4) "
     ]
    }
   ],
   "source": [
    "EPISODES = 1000\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # print(f\"Episode {episode + 1 }/{EPISODES}:\")\n",
    "    nsarsa(world, 5, (0, 0))\n",
    "    \n",
    "highest_action = np.full((world.width, world.height), '      ')\n",
    "for x in range(world.width):\n",
    "    for y in range(world.height):\n",
    "        highest_action[x, y] = ACTIONS[np.argmax(Q[x, y])]\n",
    "highest_action[7, 7] = 'GOAL'\n",
    "print(pd.DataFrame(highest_action.T))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
